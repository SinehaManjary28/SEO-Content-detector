{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfa76a4",
   "metadata": {},
   "source": [
    "# SEO Content Quality & Duplicate Detector\n",
    "\n",
    "This notebook contains a complete pipeline template for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4140d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: textstat in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.10)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyphen in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textstat) (65.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 lxml scikit-learn pandas numpy textstat sentence-transformers joblib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6bb10f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "print('Imports ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a897f9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\\data.csv shape= (81, 2)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path('data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "DATA_CSV = DATA_DIR / 'data.csv'\n",
    "EXTRACTED_CSV = DATA_DIR / 'extracted_content.csv'\n",
    "FEATURES_CSV = DATA_DIR / 'features.csv'\n",
    "DUPLICATES_CSV = DATA_DIR / 'duplicates.csv'\n",
    "\n",
    "if DATA_CSV.exists():\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "    print('Loaded', DATA_CSV, 'shape=', df.shape)\n",
    "else:\n",
    "    print('Please add data/data.csv with url and html_content columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c038f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_body_from_html(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        title = soup.title.get_text(strip=True) if soup.title else ''\n",
    "        main = soup.find('article') or soup.find('main')\n",
    "        if main:\n",
    "            parts = [p.get_text(separator=' ', strip=True) for p in main.find_all('p')]\n",
    "            body = ' '.join(parts)\n",
    "        else:\n",
    "            parts = [p.get_text(separator=' ', strip=True) for p in soup.find_all('p')]\n",
    "            body = ' '.join(parts)\n",
    "        if not body:\n",
    "            body = soup.get_text(separator=' ', strip=True)\n",
    "        body = re.sub(r'\\s+', ' ', body).strip()\n",
    "        return title, body\n",
    "    except Exception:\n",
    "        return '', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36fcddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data\\extracted_content.csv\n"
     ]
    }
   ],
   "source": [
    "if 'df' in globals() and 'html_content' in df.columns:\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        url = r.get('url','')\n",
    "        html = r.get('html_content','') or ''\n",
    "        title, body = extract_title_and_body_from_html(html)\n",
    "        rows.append({'url': url, 'title': title, 'body_text': body, 'word_count': len(body.split())})\n",
    "    extracted_df = pd.DataFrame(rows)\n",
    "    extracted_df.to_csv(EXTRACTED_CSV, index=False)\n",
    "    print('Saved', EXTRACTED_CSV)\n",
    "else:\n",
    "    print('No html_content column found in data.csv; run scraping or provide html_content.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0ab48c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved features to data\\features.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>326</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.816181</td>\n",
       "      <td>cyber|alliance|cyber management|management all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Cybersecurity is gaining more importance globa...</td>\n",
       "      <td>1578</td>\n",
       "      <td>78</td>\n",
       "      <td>38.946453</td>\n",
       "      <td>varonis|access|data|security|app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Cybersecurity is inextricably tied to the tech...</td>\n",
       "      <td>946</td>\n",
       "      <td>61</td>\n",
       "      <td>53.698274</td>\n",
       "      <td>password|passphrase|authentication|protect|device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cyberspace is particularly difficult to secure...</td>\n",
       "      <td>489</td>\n",
       "      <td>22</td>\n",
       "      <td>9.653990</td>\n",
       "      <td>cisa|cybersecurity|cyber|nation|cybersecurity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00|тhe gartner|тhe|čapek films|čapek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           body_text  word_count  \\\n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326   \n",
       "1  Cybersecurity is gaining more importance globa...        1578   \n",
       "2  Cybersecurity is inextricably tied to the tech...         946   \n",
       "3  Cyberspace is particularly difficult to secure...         489   \n",
       "4                                                NaN           0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  \\\n",
       "0               6            -6.816181   \n",
       "1              78            38.946453   \n",
       "2              61            53.698274   \n",
       "3              22             9.653990   \n",
       "4               0             0.000000   \n",
       "\n",
       "                                        top_keywords  \n",
       "0  cyber|alliance|cyber management|management all...  \n",
       "1                   varonis|access|data|security|app  \n",
       "2  password|passphrase|authentication|protect|device  \n",
       "3  cisa|cybersecurity|cyber|nation|cybersecurity ...  \n",
       "4               00|тhe gartner|тhe|čapek films|čapek  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature engineering (readability + TF-IDF top keywords extraction)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Optional readability library\n",
    "try:\n",
    "    import textstat\n",
    "    _HAS_TEXTSTAT = True\n",
    "except Exception:\n",
    "    _HAS_TEXTSTAT = False\n",
    "\n",
    "def estimate_syllables(word):\n",
    "    word = word.lower()\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    if word and word[0] in vowels:\n",
    "        count += 1\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count = max(1, count - 1)\n",
    "    return max(1, count)\n",
    "\n",
    "def compute_readability(text):\n",
    "    \"\"\"Calculate Flesch Reading Ease score (fallback if textstat unavailable).\"\"\"\n",
    "    if _HAS_TEXTSTAT:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    sents = max(1, len(sent_tokenize(text)))\n",
    "    words = text.split()\n",
    "    words_count = max(1, len(words))\n",
    "    sylls = sum(estimate_syllables(w) for w in words)\n",
    "    asl = words_count / sents\n",
    "    asw = sylls / words_count\n",
    "    score = 206.835 - 1.015 * asl - 84.6 * asw\n",
    "    return score\n",
    "\n",
    "# ---- Feature Extraction ----\n",
    "if EXTRACTED_CSV.exists():\n",
    "    ex = pd.read_csv(EXTRACTED_CSV)\n",
    "\n",
    "    # Basic text metrics\n",
    "    ex['sentence_count'] = ex['body_text'].fillna('').apply(lambda t: len(sent_tokenize(t)) if t.strip() else 0)\n",
    "    ex['word_count'] = ex['body_text'].fillna('').apply(lambda t: len(t.split()))\n",
    "    ex['flesch_reading_ease'] = ex['body_text'].fillna('').apply(\n",
    "        lambda t: compute_readability(t) if t.strip() else 0.0\n",
    "    )\n",
    "\n",
    "    # Build TF-IDF model\n",
    "    corpus = ex['body_text'].fillna('').astype(str).tolist()\n",
    "    if any(corpus):\n",
    "        vect = TfidfVectorizer(\n",
    "            max_df=0.8, min_df=1, stop_words='english', ngram_range=(1, 2)\n",
    "        )\n",
    "        X = vect.fit_transform(corpus)\n",
    "        feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "        top_keywords_list = []\n",
    "        for i in range(X.shape[0]):\n",
    "            row = X[i].toarray().ravel()   # convert sparse matrix row to dense array\n",
    "            top_idx = row.argsort()[-5:][::-1]\n",
    "            keywords = '|'.join(feature_names[top_idx])\n",
    "            top_keywords_list.append(keywords)\n",
    "\n",
    "        ex['top_keywords'] = top_keywords_list\n",
    "        tfidf_matrix = X\n",
    "        tfidf_vect = vect\n",
    "    else:\n",
    "        ex['top_keywords'] = ''\n",
    "        tfidf_matrix = None\n",
    "        tfidf_vect = None\n",
    "\n",
    "    # Save features\n",
    "    ex.to_csv(FEATURES_CSV, index=False)\n",
    "    print('✅ Saved features to', FEATURES_CSV)\n",
    "    display(ex.head())\n",
    "else:\n",
    "    print('⚠️ Run the extraction step first (extracted_content.csv missing).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e8a2d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved duplicates to data\\duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "# Duplicate detection (cosine similarity)\n",
    "if 'tfidf_matrix' in globals():\n",
    "    sim = cosine_similarity(tfidf_matrix)\n",
    "    threshold = 0.80\n",
    "    pairs = []\n",
    "    n = sim.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if sim[i,j] >= threshold:\n",
    "                pairs.append({'url1': ex.loc[i,'url'], 'url2': ex.loc[j,'url'], 'similarity': float(sim[i,j])})\n",
    "    pd.DataFrame(pairs).to_csv(DUPLICATES_CSV, index=False)\n",
    "    print('Saved duplicates to', DUPLICATES_CSV)\n",
    "else:\n",
    "    print('TF-IDF not available; skip duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "376a5420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.67      1.00      0.80         2\n",
      "         Low       0.94      1.00      0.97        15\n",
      "      Medium       1.00      0.75      0.86         8\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.87      0.92      0.87        25\n",
      "weighted avg       0.94      0.92      0.92        25\n",
      "\n",
      "Saved model to models/quality_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Labeling & Model training\n",
    "if 'ex' in globals():\n",
    "    def quality_label(row):\n",
    "        wc = row['word_count']; fr = row['flesch_reading_ease']\n",
    "        if (wc > 1500) and (50 <= fr <= 70):\n",
    "            return 'High'\n",
    "        if (wc < 500) or (fr < 30):\n",
    "            return 'Low'\n",
    "        return 'Medium'\n",
    "    ex['quality_label'] = ex.apply(quality_label, axis=1)\n",
    "    features = ex[['word_count','sentence_count','flesch_reading_ease']].fillna(0)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(ex['quality_label'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    joblib.dump(clf, 'models/quality_model.pkl')\n",
    "    joblib.dump(le, 'models/label_encoder.pkl')\n",
    "    print('Saved model to models/quality_model.pkl')\n",
    "else:\n",
    "    print('Features missing; cannot train model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c769d",
   "metadata": {},
   "source": [
    "# Live content analyzer for any webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0847da93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Live SEO Content Analysis Result:\n",
      "{\n",
      "  \"url\": \"https://www.bbc.com/news/technology\",\n",
      "  \"title\": \"BBC Innovation | Technology, Health, Environment, AI\",\n",
      "  \"word_count\": 997,\n",
      "  \"sentence_count\": 63,\n",
      "  \"flesch_reading_ease\": 54.81,\n",
      "  \"quality_label\": \"Medium\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "def analyze_url(url):\n",
    "    \"\"\"Fetches a webpage, extracts features, and predicts content quality.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (compatible; SEO-Content-Detector/1.0)'}\n",
    "        r = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return {'url': url, 'error': f'HTTP {r.status_code}'}\n",
    "\n",
    "        # ---- Extract title and body ----\n",
    "        title, body = extract_title_and_body_from_html(r.text)\n",
    "        if not body.strip():\n",
    "            return {'url': url, 'error': 'Empty body text extracted'}\n",
    "\n",
    "        # ---- Compute features ----\n",
    "        wc = len(body.split())\n",
    "        sc = len([s for s in body.split('.') if s.strip()])\n",
    "        fr = compute_readability(body)\n",
    "\n",
    "        # ---- Predict using trained model ----\n",
    "        model_path = Path('models/quality_model.pkl')\n",
    "        encoder_path = Path('models/label_encoder.pkl')\n",
    "\n",
    "        if model_path.exists() and encoder_path.exists():\n",
    "            clf = joblib.load(model_path)\n",
    "            le = joblib.load(encoder_path)\n",
    "            pred = clf.predict([[wc, sc, fr]])\n",
    "            label = le.inverse_transform(pred)[0]\n",
    "        else:\n",
    "            label = \"Unknown (model not found)\"\n",
    "\n",
    "        # ---- Build result ----\n",
    "        result = {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'word_count': wc,\n",
    "            'sentence_count': sc,\n",
    "            'flesch_reading_ease': round(fr, 2),\n",
    "            'quality_label': label\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'url': url, 'error': str(e)}\n",
    "\n",
    "# ------------------------------\n",
    "# Testing the live analyzer\n",
    "# ------------------------------\n",
    "test_url = \"https://www.bbc.com/news/technology\" \n",
    "result = analyze_url(test_url)\n",
    "\n",
    "# Pretty print the output\n",
    "print(\"\\n Live SEO Content Analysis Result:\")\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029d4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
